%YAML:1.0

#--------------------------------------------------------------------------------------------
# Camera Parameters. Adjust them!
#--------------------------------------------------------------------------------------------
Camera.type: "PinHole"

# Camera calibration and distortion parameters (OpenCV) 
Camera.fx: 1724.778859
Camera.fy: 1723.969299
Camera.cx: 489.853476
Camera.cy: 250.216425

Camera.k1: 0.0
Camera.k2: 0.0
Camera.p1: 0.0
Camera.p2: 0.0
Camera.k3: 0.0

Camera.bFishEye: 0

Camera.width: 1032
Camera.height: 772

# Camera frames per second 
Camera.fps: 10.0

# IR projector baseline times fx (aprox.)
Camera.bf: 100.0
# Camera.bf is the horizontal focal length (in pixels) multiplied by the baseline (in meters). [https://github.com/raulmur/ORB_SLAM2/issues/89]
# TODO: I don't get how bf should be set as this is not a stereo setup

# Color order of the images (0: BGR, 1: RGB. It is ignored if images are grayscale)
Camera.RGB: 1

# Close/Far threshold. Baseline times.
Stereo.ThDepth: 700.0

# Deptmap values factor 
RGBD.DepthMapFactor: 1000.0 
# RGBL does not use this parameter, it's just that the code still checks if this has been set

#--------------------------------------------------------------------------------------------
# LiDAR Parameters
#--------------------------------------------------------------------------------------------
# Rotation Translation Matrix from LiDAR to Camera Frame of reference
LiDAR.Tr11: 0.06975647
LiDAR.Tr12: -0.99756405
LiDAR.Tr13: 0.0
LiDAR.Tr14: 0.17462933
LiDAR.Tr21: 0.08694344
LiDAR.Tr22: 0.00607968
LiDAR.Tr23: -0.9961947
LiDAR.Tr24: -0.02189797
LiDAR.Tr31: 0.99376802
LiDAR.Tr32: 0.06949103
LiDAR.Tr33: 0.08715574
LiDAR.Tr34: 0.04802163

# Option for Upsampling
#LiDAR.Method: "NearestNeighborPixel"
#LiDAR.Method: "AverageFiltering"
LiDAR.Method: "InverseDilation"

# Minimum and Maximum Distance to be considered during pointcloud projection
LiDAR.min_dist: 5.0
LiDAR.max_dist: 200.0

# Method Specific Options
# Only Consider the Options for the method you selected, all others may be ignored
# Do not delete entries that are not required, just ignore them.
int main(int argc, char **argv)
{
    if (argc != 4)
    {
        cerr << endl
             << "Usage: ./rgbl_kitti path_to_vocabulary path_to_settings path_to_sequence" << endl;
        return 1;
    }
    
    // Retrieve paths to images
    vector<string> vstrImageFilenamesRGB;
    vector<string> vstrPcdFilenames;
    vector<double> vTimestamps;
    string DatasetPath = argv[3];   
    
    LoadImages(DatasetPath, vstrImageFilenamesRGB, vstrPcdFilenames, vTimestamps);
    cout << "Done Loading Images" << endl;

    // Check consistency in the number of images and depthmaps
    int nImages = vstrImageFilenamesRGB.size();
    if (vstrImageFilenamesRGB.empty())
    {
        cerr << endl
             << "No images found in provided path." << endl;
        return 1;
    }
    else if (vstrPcdFilenames.size() != vstrImageFilenamesRGB.size())
    {
        cerr << endl
             << "Different number of images for rgb and pointclouds." << endl;
        return 1;
    }

    // Create SLAM system. It initializes all system threads and gets ready to process frames.
    ORB_SLAM3::System SLAM(argv[1], argv[2], ORB_SLAM3::System::RGBL, true);

    // Vector for tracking time statistics
    vector<float> vTimesTrack;
    vTimesTrack.resize(nImages);

    cout << endl
         << "-------" << endl;
    cout << "Start processing sequence ..." << endl;
    cout << "Images in the sequence123: " << nImages << endl
         << endl;

    // Main loop
    cv::Mat imRGB, pcd;
    bool SuccessPcdLoad;
    for (int ni = 0; ni < nImages; ni++)
    {
        // Read image and depthmap from file
        cout << "RGB Image: " << vstrImageFilenamesRGB[ni] << endl;
        // cout << "Depth Image: " << vstrImageFilenamesD[ni] << endl;
        imRGB = cv::imread(vstrImageFilenamesRGB[ni], cv::IMREAD_UNCHANGED);
        cout << "HALLO 0" << endl;
        SuccessPcdLoad = LoadPointcloudBinaryMat(vstrPcdFilenames[ni], pcd);
        cout << "HALLO 00 " << endl;
        double tframe = vTimestamps[ni];
        cout << "HALLO 000 " << endl;
        if (!SuccessPcdLoad){
            std::cout << "Skipping frame, could not load pcd: " << vstrPcdFilenames[ni] << std::endl;
            continue;
        }

        if (imRGB.empty())
        {
            cerr << endl
                 << "Failed to load image at: "
                 << string(DatasetPath) << "/" << vstrImageFilenamesRGB[ni] << endl;
            return 1;
        }

        cout << "HALLO 1 " << endl;

        std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();


        // Pass the image to the SLAM system
        SLAM.TrackRGBL(imRGB, pcd, tframe);


        std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();


        double ttrack = std::chrono::duration_cast<std::chrono::duration<double>>(t2 - t1).count();

        vTimesTrack[ni] = ttrack;
        cout << "HALLO 2 " << endl;

        // // Wait to load the next frame
        // double T = 0;
        // if (ni < nImages - 1)
        //     T = vTimestamps[ni + 1] - tframe;
        // else if (ni > 0)
        //     T = tframe - vTimestamps[ni - 1];

        // if (ttrack < T)
        //     usleep((T - ttrack) * 1e6);
    }

    // Stop all threads
    SLAM.Shutdown();

    // Tracking time statistics
    sort(vTimesTrack.begin(), vTimesTrack.end());
    float totaltime = 0;
    for (int ni = 0; ni < nImages; ni++)
    {
        totaltime += vTimesTrack[ni];
    }
    cout << "-------" << endl
         << endl;
    cout << "median tracking time: " << vTimesTrack[nImages / 2] << endl;
    cout << "mean tracking time: " << totaltime / nImages << endl;

    // Save camera trajectory
    SLAM.SaveTrajectoryKITTI("CameraTrajectory.txt");

    return 0;
}
# For Nearest Neighbor on Pixel Level
LiDAR.MethodNearestNeighborPixel.SearchDistance: 7.0

# For Average Filtering
LiDAR.MethodAverageFiltering.bDoDilationPreprocessing: 1
LiDAR.MethodAverageFiltering.DilationPreprocessing_KernelType: "Diamond"
LiDAR.MethodAverageFiltering.DilationPreprocessing_KernelSize: 3.0
LiDAR.MethodAverageFiltering.KernelSize: 5.0

# For Inverse Dilation
LiDAR.MethodInverseDilation.KernelType: "Diamond"
LiDAR.MethodInverseDilation.KernelSize: 7.0

#--------------------------------------------------------------------------------------------
# ORB Parameters
#--------------------------------------------------------------------------------------------

# ORB Extractor: Number of features per image
ORBextractor.nFeatures: 2000

# ORB Extractor: Scale factor between levels in the scale pyramid 	
ORBextractor.scaleFactor: 1.2

# ORB Extractor: Number of levels in the scale pyramid	
ORBextractor.nLevels: 8

# ORB Extractor: Fast threshold
# Image is divided in a grid. At each cell FAST are extracted imposing a minimum response.
# Firstly we impose iniThFAST. If no corners are detected we impose a lower value minThFAST
# You can lower these values if your images have low contrast			
ORBextractor.iniThFAST: 12
ORBextractor.minThFAST: 7

#--------------------------------------------------------------------------------------------
# Viewer Parameters
#--------------------------------------------------------------------------------------------
Viewer.KeyFrameSize: 0.6
Viewer.KeyFrameLineWidth: 2
Viewer.GraphLineWidth: 1
Viewer.PointSize:2
Viewer.CameraSize: 0.7
Viewer.CameraLineWidth: 3
Viewer.ViewpointX: 0
Viewer.ViewpointY: -100
Viewer.ViewpointZ: -0.1
Viewer.ViewpointF: 2000

